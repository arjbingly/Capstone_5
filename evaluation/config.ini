[llm]
model_name : Llama-2-13b-chat
# meta-llama/Llama-2-70b-chat-hf Mixtral-8x7B-Instruct-v0.1
quantization : Q5_K_M
device_map : auto
task : text-generation
max_new_tokens : 1024
temperature : 0.1
n_batch : 1024
n_ctx : 6000
n_gpu_layers : -1
# The number of layers to put on the GPU. Mixtral-18
std_out : True

[chroma]
host : localhost
port : 8001
collection_name : sec-10-q-v1
# embedding_type : sentence-transformers
# embedding_model : "all-mpnet-base-v2"
embedding_type : instructor-embedding
embedding_model : hkunlp/instructor-xl

[text_splitter]
chunk_size : 5000
chunk_overlap : 400

[multivec_retriever]
# store_path: data/docs
store_path : ${data:data_path}/doc_store
# namespace: UUID(8c9040b0-b5cd-4d7c-bc2e-737da1b24ebf)
namespace : 8c9040b0b5cd4d7cbc2e737da1b24ebf
id_key : doc_id

[parser]
single_text_out : True
strategy : hi_res
infer_table_structure : True
extract_images : True
image_output_dir : None
add_captions_to_text : True
add_captions_to_blocks : True
table_as_html : True

[data]
data_path : /home/ubuntu/Capstone_5/evaluation/data
